# RAG Based AI Teaching Assistant for Educational Videos

This project is an AI-powered teaching assistant that processes educational videos to enable interactive question answering based on the video content. It converts videos to audio, transcribes the audio to text, cleans and preprocesses the transcription data, generates embeddings, and provides a question-answering interface using a local language model.

## Project Structure

- `main.py` - The main script orchestrating the entire processing pipeline and question answering.
- `video_tranformer.py` - Converts videos in the `videos/` directory to audio files in the `audios/` directory.
- `audio_transformer.py` - Transcribes audio files to JSON format using the Whisper model, saving results in `json_data/`.
- `json_processor.py` - Cleans and preprocesses the transcription JSON files, saving cleaned data in `clean_json_data/`.
- `data_processor.py` - Generates embeddings for the cleaned text chunks and saves them in a dataframe (`dataframe.joblib`).
- `get_output.py` - Provides an interactive question-answering interface using embeddings and a local language model.
- `videos/` - Directory containing input video files.
- `audios/` - Directory where extracted audio files are saved.
- `json_data/` - Directory containing raw transcription JSON files.
- `clean_json_data/` - Directory containing cleaned and preprocessed JSON data.

## Workflow

1. **Video to Audio Conversion**  
   Videos in the `videos/` folder are converted to audio files (`.mp3`) using `ffmpeg` via `video_tranformer.py`.

2. **Audio Transcription to JSON**  
   Audio files are transcribed to JSON format using OpenAI's Whisper model in `audio_transformer.py`.

3. **JSON Cleaning and Preprocessing**  
   The raw transcription JSON files are cleaned and structured for further processing by `json_processor.py`.

4. **Embedding Generation and Dataframe Creation**  
   Text chunks from cleaned JSON files are sent to a local embedding API to generate embeddings, which are saved in a pandas dataframe by `data_processor.py`.

5. **Question Answering Interface**  
   Users can ask questions related to the video content. The system computes similarity between the question and video content embeddings, then generates a natural language response using a local language model API in `get_output.py`.

## Requirements

- Python 3.x
- `ffmpeg` installed and accessible in system PATH
- Python packages: `joblib`, `pandas`, `requests`, `scikit-learn`, `numpy`, `whisper` (OpenAI Whisper)
- Local embedding API running at `http://localhost:11434/api/embed`
- Local language model API running at `http://localhost:11434/api/generate`
- [Ollama](https://ollama.com/) installed for running local models
- Pull the following models in Ollama for the project to work:
  - `bge-m3`
  - `llama3.2`

## Usage

1. Place your educational videos in the `videos/` directory.
2. Run the main script to process videos and generate embeddings:
   ```bash
   python main.py
   ```
3. After processing, you will be prompted to ask questions related to the video content.
4. Enter your question and receive a helpful response with references to relevant video timestamps.
5. The response is also saved in `response.txt`.

## Output

- `response.txt` contains the latest answer generated by the AI teaching assistant.
- Interactive Q&A is done via the command line prompt after running `main.py`.

## Notes

- Ensure `ffmpeg` is installed and accessible from the command line.
- The local embedding and language model APIs must be running for the system to function correctly.
- The Whisper model used is the "small" variant for transcription.
- The system currently supports English language videos.
- For best results, keep the videos focused on educational content.

---

This project enables an interactive learning experience by leveraging video content and AI-powered natural language understanding.
